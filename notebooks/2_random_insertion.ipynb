{"cells":[{"cell_type":"markdown","metadata":{"id":"wEPhzQk9sbYz"},"source":["# Emotional Classification Pipeline"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98215,"status":"ok","timestamp":1750006819650,"user":{"displayName":"Tech Odyssey","userId":"11914994264273208701"},"user_tz":-120},"id":"Rj0OYqIlxDxQ","outputId":"112f0bca-723b-41d7-e7e4-6e05dfae5c0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Collecting datasets\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Collecting fasttext\n","  Downloading fasttext-0.9.3.tar.gz (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow\u003e=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill\u003c0.3.9,\u003e=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests\u003e=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm\u003e=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess\u003c0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Collecting fsspec\u003c=2025.3.0,\u003e=2023.1.0 (from fsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: huggingface-hub\u003e=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Collecting pybind11\u003e=2.2 (from fasttext)\n","  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools\u003e=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (3.11.15)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003e=0.24.0-\u003edatasets) (4.14.0)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub\u003e=0.24.0-\u003edatasets) (1.1.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.32.2-\u003edatasets) (3.4.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.32.2-\u003edatasets) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.32.2-\u003edatasets) (2.4.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.32.2-\u003edatasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets) (2.9.0.post0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.3.2)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (25.3.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.6.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (6.4.4)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (0.3.1)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-\u003efsspec[http]\u003c=2025.3.0,\u003e=2023.1.0-\u003edatasets) (1.20.0)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas-\u003edatasets) (1.17.0)\n","Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313509 sha256=e500587e7865e201371b0d81f290a9205c12cbf7344cb6faa8c343b27aea7100\n","  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n","Successfully built fasttext\n","Installing collected packages: pybind11, fsspec, fasttext, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.14.4\n","    Uninstalling datasets-2.14.4:\n","      Successfully uninstalled datasets-2.14.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.6.0 fasttext-0.9.3 fsspec-2025.3.0 pybind11-2.13.6\n"]}],"source":["!pip install -U datasets fasttext"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzBpKJj4DmFG","outputId":"fa6b9d6e-3a16-40bf-c1b3-8a588f738228"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2025-06-15 17:00:19--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.af.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.111, 13.226.210.25, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2567394808 (2.4G) [application/zip]\n","Saving to: ‘wiki.af.zip’\n","\n","wiki.af.zip         100%[===================\u003e]   2.39G   167MB/s    in 19s     \n","\n","2025-06-15 17:00:38 (130 MB/s) - ‘wiki.af.zip’ saved [2567394808/2567394808]\n","\n","Archive:  wiki.af.zip\n","  inflating: wiki.af.vec             \n","  inflating: wiki.af.bin             \n","--2025-06-15 17:01:30--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ha.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.15, 13.226.210.78, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2233056492 (2.1G) [application/zip]\n","Saving to: ‘wiki.ha.zip’\n","\n","wiki.ha.zip         100%[===================\u003e]   2.08G  31.4MB/s    in 20s     \n","\n","2025-06-15 17:01:50 (105 MB/s) - ‘wiki.ha.zip’ saved [2233056492/2233056492]\n","\n","Archive:  wiki.ha.zip\n","  inflating: wiki.ha.vec             \n","  inflating: wiki.ha.bin             \n","--2025-06-15 17:02:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.sw.zip\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.111, 13.226.210.78, 13.226.210.25, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.111|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2413905216 (2.2G) [application/zip]\n","Saving to: ‘wiki.sw.zip’\n","\n","wiki.sw.zip         100%[===================\u003e]   2.25G   100MB/s    in 17s     \n","\n","2025-06-15 17:02:40 (136 MB/s) - ‘wiki.sw.zip’ saved [2413905216/2413905216]\n","\n","Archive:  wiki.sw.zip\n","  inflating: wiki.sw.vec             \n","  inflating: wiki.sw.bin             "]}],"source":["# Get fastText pretrained vectors\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.af.zip\n","!unzip wiki.af.zip\n","\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.ha.zip\n","!unzip wiki.ha.zip\n","\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.sw.zip\n","!unzip wiki.sw.zip"]},{"cell_type":"markdown","metadata":{"id":"Rgm8PVAZ2r-z"},"source":["## Install Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jth7Lzasvwq"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from datasets import load_dataset, concatenate_datasets, Dataset\n","from peft import LoraConfig, get_peft_model\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","import plotly.express as px\n","import pandas as pd\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, hamming_loss\n","from sklearn.model_selection import train_test_split\n","import torch\n","import random\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","import fasttext\n","from numpy import dot\n","from numpy.linalg import norm\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLla7bzXLtdF"},"outputs":[],"source":["def set_all_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # set_seed(seed)  # HuggingFace transformers seed\n"]},{"cell_type":"markdown","metadata":{"id":"48KIQXaQ2r-0"},"source":["## Define Pipeline Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIi3-uyq2r-1"},"outputs":[],"source":["model_name = \"Davlan/afro-xlmr-small\"  # Base model name\n","run_baseline = False # Set to True to run the baseline model (skip data augmentation)\n","languages = [\"afr\", \"hau\", \"swa\"] # List of languages to include, can be run on all languages/specific ones\n","# languages = [\"afr\"] # List of languages to include, can be run on all languages/specific ones\n","seed = 42 # Set a seed for reproducibility\n","\n","emotion_labels = {\n","    \"joy\": 0,\n","    \"anger\": 1,\n","    \"fear\": 2,\n","    \"sadness\": 3,\n","    \"disgust\": 4,\n","    \"surprise\": 5,\n","    \"neutral\": 6,\n","}\n","\n","set_all_seeds(seed)"]},{"cell_type":"markdown","metadata":{"id":"BvE3PQer2r-1"},"source":["## Load Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHn6jIKe2r-2"},"outputs":[],"source":["def load_datasets(languages):\n","    # Load the datasets for the specified languages\n","    # datasets are in the form:\n","    # {\n","    #     \"lang1\": dataset1,\n","    #     \"lang2\": dataset2,\n","    #     ...\n","    # }\n","\n","    datasets = {}\n","    for lang in languages:\n","        print(f\"Loading dataset for {lang}...\")\n","        hf_dataset = load_dataset(\"brighter-dataset/BRIGHTER-emotion-categories\", lang)\n","\n","        combined_hf_dataset = concatenate_datasets([\n","            hf_dataset[\"train\"],\n","            hf_dataset[\"test\"],\n","            hf_dataset[\"dev\"]\n","        ])\n","        datasets[lang] = combined_hf_dataset\n","        print(f\"Loaded {len(combined_hf_dataset)} samples for {lang}.\")\n","\n","\n","    return datasets\n","\n","\n","datasets = load_datasets(languages)"]},{"cell_type":"markdown","metadata":{"id":"NYavM7_6nKrd"},"source":["## Load FastText Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9Ht-y7lnKW5"},"outputs":[],"source":["swahili_model = fasttext.load_model('wiki.sw.bin')\n","hausa_model = fasttext.load_model('wiki.ha.bin')\n","afrikaans_model = fasttext.load_model('wiki.af.bin')\n","\n","language_models = {\n","    \"swa\": swahili_model,\n","    \"hau\": hausa_model,\n","    \"afr\": afrikaans_model\n","}"]},{"cell_type":"markdown","metadata":{"id":"-MDQ62tn2r-2"},"source":["## Preprocess Dataset\n","- Map empty emotions to neutral\n","- Split in to train, validation and test sets using stratified sampling by emotion label\n","- Add multi-hot encoded label column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7gZ0IFK2r-3"},"outputs":[],"source":["def preprocess_datasets(datasets, emotion_to_id_dict):\n","    # datasets are in form\n","    # {\n","    #     \"lang1\": {\n","    #         \"train\": \u003ctrain_dataset\u003e,\n","    #         \"test\": \u003ctest_dataset\u003e,\n","    #         \"validation\": \u003cvalidation_dataset\u003e\n","    #     },\n","    #     \"lang2\": {\n","    #         \"train\": \u003ctrain_dataset\u003e,\n","    #         \"test\": \u003ctest_dataset\u003e,\n","    #         \"validation\": \u003cvalidation_dataset\u003e\n","    #     },\n","    #     ...\n","    # }\n","\n","    preprocessed_datasets = {}\n","    for lang, dataset in datasets.items():\n","        print(f\"Preprocessing dataset for {lang}...\")\n","\n","        # Replace empty labels with \"neutral\"\n","        dataset = dataset.map(\n","            lambda example: {\n","                **example,\n","                \"emotions\": example[\"emotions\"] if len(example[\"emotions\"]) \u003e 0 else [\"neutral\"]\n","            }\n","        )\n","\n","        # Convert the list of emotions to a string representation for stratification\n","        def categorise_emotions(emotions_list):\n","            if not emotions_list:\n","                return \"none\"\n","            elif len(emotions_list) == 1:\n","                return emotions_list[0]\n","            else:\n","                return \"multiple\"\n","\n","        # Add a new column for stratification\n","        dataset = dataset.map(\n","            lambda example: {\"emotion_key\": categorise_emotions(example[\"emotions\"])}\n","        )\n","\n","        # Now split using train_test_split from scikit-learn\n","        train_size = 0.8\n","        test_size = 0.1\n","        dev_size = 0.1\n","\n","        # Convert to pandas for easier handling with sklearn\n","        df = dataset.to_pandas()\n","\n","        print(f\"Stratifying by emotion...\")\n","        # First split to get train and the rest\n","        train_df, temp_df = train_test_split(\n","            df,\n","            train_size=train_size,\n","            stratify=df[\"emotion_key\"],\n","            random_state=seed\n","        )\n","\n","        # Then split the rest into test and dev\n","        test_df, dev_df = train_test_split(\n","            temp_df,\n","            train_size=test_size/(test_size + dev_size),\n","            stratify=temp_df[\"emotion_key\"],\n","            random_state=seed\n","        )\n","\n","        small_train, left_over = train_test_split(\n","            train_df,\n","            train_size=0.2,\n","            stratify=train_df[\"emotion_key\"],\n","            random_state=seed\n","        )\n","\n","        small_train_dataset = Dataset.from_pandas(small_train)\n","        train_dataset = Dataset.from_pandas(train_df)\n","        test_dataset = Dataset.from_pandas(test_df)\n","        dev_dataset = Dataset.from_pandas(dev_df)\n","\n","\n","        num_labels = len(emotion_to_id_dict)\n","\n","        # Create a new column 'labels' containing multi-hot encoded labels\n","        def map_emotions_to_labels(example):\n","            # Initialise a zero array for all emotions\n","            labels = [0.0] * num_labels  # Use floats instead of integers\n","\n","            # Handle both string format and list format\n","            emotions_list = example['emotions']\n","\n","            # Set 1 for each emotion present in the example\n","            for emotion in emotions_list:\n","                labels[emotion_to_id_dict[emotion]] = 1.0\n","\n","            example['labels'] = labels\n","            return example\n","\n","        # Apply the mapping function to the train, test, and dev datasets\n","        small_train_dataset = small_train_dataset.map(map_emotions_to_labels)\n","        train_dataset = train_dataset.map(map_emotions_to_labels)\n","        test_dataset = test_dataset.map(map_emotions_to_labels)\n","        dev_dataset = dev_dataset.map(map_emotions_to_labels)\n","\n","\n","        print(f\"Dataset for {lang} split with {len(train_dataset)} train, {len(test_dataset)} test, and {len(dev_dataset)} validation samples.\")\n","\n","        # Store the datasets in the dictionary\n","        preprocessed_datasets[lang] = {\n","            # \"train\": small_train_dataset,\n","            \"train\": train_dataset,\n","            \"test\": test_dataset,\n","            \"validation\": dev_dataset\n","        }\n","    return preprocessed_datasets\n","\n","\n","preprocessed_datasets = preprocess_datasets(datasets.copy(), emotion_labels)"]},{"cell_type":"markdown","metadata":{"id":"cz0XJ5Bf2r-4"},"source":["# Apply data augmentation techique\n","\u003e Skips if running baseline model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Zu8kQU8KSC6Y"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","-------------- Applying data augmentation for afr --------------\n","Total number of training instances before augmentation: 2838\n","Training distribution before augmentation:\n","fear: 301 - 11%\n","Label fear is underrepresented\n","anger: 177 - 6%\n","Label anger is extremely underrepresented\n","disgust: 133 - 5%\n","Label disgust is extremely underrepresented\n","neutral: 1075 - 38%\n","Label neutral is over represented\n","Label neutral is well represented\n","joy: 1112 - 39%\n","Label joy is over represented\n","Label joy is well represented\n","sadness: 439 - 15%\n","Label sadness is moderately represented\n","Augmenting label: 'fear'...\n","Generating approx. 602 samples for 'fear'...\n","Augmenting label: 'anger'...\n","Generating approx. 354 samples for 'anger'...\n","Augmenting label: 'disgust'...\n","Generating approx. 266 samples for 'disgust'...\n","Augmenting label: 'neutral'...\n","Label 'neutral' already has 1075 samples, no augmentation needed.\n","Augmenting label: 'neutral'...\n","Generating approx. 322 samples for 'neutral'...\n","Augmenting label: 'joy'...\n","Label 'joy' already has 1112 samples, no augmentation needed.\n","Augmenting label: 'joy'...\n","Generating approx. 333 samples for 'joy'...\n","Augmenting label: 'sadness'...\n","Generating approx. 439 samples for 'sadness'...\n","Total number of training instances after augmentation: 5154\n","\n","Training distribution after augmentation for afr\n","fear: 1150 - 22\n","anger: 862 - 17\n","disgust: 712 - 14\n","neutral: 1397 - 27\n","joy: 1507 - 29\n","sadness: 1475 - 29\n","\n","-------------- Applying data augmentation for hau --------------\n","Total number of training instances before augmentation: 4013\n","Training distribution before augmentation:\n","neutral: 517 - 13%\n","Label neutral is moderately represented\n","fear: 608 - 15%\n","Label fear is well represented\n","sadness: 1223 - 30%\n","Label sadness is over represented\n","Label sadness is well represented\n","anger: 773 - 19%\n","Label anger is well represented\n","disgust: 623 - 16%\n","Label disgust is well represented\n","joy: 597 - 15%\n","Label joy is well represented\n","surprise: 652 - 16%\n","Label surprise is well represented\n","Augmenting label: 'neutral'...\n","Generating approx. 517 samples for 'neutral'...\n","No words in model\n","No words in model\n","No words in model\n","No words in model\n","Augmenting label: 'fear'...\n","Generating approx. 182 samples for 'fear'...\n","Augmenting label: 'sadness'...\n","Label 'sadness' already has 1223 samples, no augmentation needed.\n","Augmenting label: 'sadness'...\n","Generating approx. 366 samples for 'sadness'...\n","Augmenting label: 'anger'...\n","Generating approx. 231 samples for 'anger'...\n","No words in model\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-12-2949429552\u003e:9: RuntimeWarning: invalid value encountered in scalar divide\n","  return dot(v1, v2) / (norm(v1) * norm(v2))\n"]},{"name":"stdout","output_type":"stream","text":["Augmenting label: 'disgust'...\n","Generating approx. 186 samples for 'disgust'...\n","Augmenting label: 'joy'...\n","Generating approx. 179 samples for 'joy'...\n","Augmenting label: 'surprise'...\n","Generating approx. 195 samples for 'surprise'...\n","Total number of training instances after augmentation: 5863\n","\n","Training distribution after augmentation for hau\n","neutral: 1028 - 18\n","fear: 923 - 16\n","sadness: 1829 - 31\n","anger: 1123 - 19\n","disgust: 888 - 15\n","joy: 805 - 14\n","surprise: 923 - 16\n","\n","-------------- Applying data augmentation for swa --------------\n","Total number of training instances before augmentation: 6176\n","Training distribution before augmentation:\n","neutral: 2749 - 45%\n","Label neutral is over represented\n","Label neutral is well represented\n","joy: 830 - 13%\n","Label joy is moderately represented\n","surprise: 1000 - 16%\n","Label surprise is well represented\n","sadness: 647 - 10%\n","Label sadness is moderately represented\n","fear: 171 - 3%\n","Label fear is extremely underrepresented\n","anger: 586 - 9%\n","Label anger is moderately represented\n","disgust: 450 - 7%\n","Label disgust is underrepresented\n","Augmenting label: 'neutral'...\n","Label 'neutral' already has 2749 samples, no augmentation needed.\n","Augmenting label: 'neutral'...\n","Generating approx. 824 samples for 'neutral'...\n","Augmenting label: 'joy'...\n","Generating approx. 830 samples for 'joy'...\n","Augmenting label: 'surprise'...\n","Generating approx. 300 samples for 'surprise'...\n","Augmenting label: 'sadness'...\n","Generating approx. 647 samples for 'sadness'...\n","Augmenting label: 'fear'...\n","Generating approx. 342 samples for 'fear'...\n","Augmenting label: 'anger'...\n","Generating approx. 586 samples for 'anger'...\n","Augmenting label: 'disgust'...\n","Generating approx. 900 samples for 'disgust'...\n","Total number of training instances after augmentation: 10522\n","\n","Training distribution after augmentation for swa\n","neutral: 3573 - 34\n","joy: 1665 - 16\n","surprise: 1445 - 14\n","sadness: 1403 - 13\n","fear: 532 - 5\n","anger: 1278 - 12\n","disgust: 1440 - 14\n"]}],"source":["def sentence_vector(sentence, model):\n","    words = sentence.split()\n","    word_vectors = [model.get_word_vector(w) for w in words if w in model]\n","    if len(word_vectors) == 0:\n","        return np.zeros(model.get_dimension())\n","    return np.mean(word_vectors, axis=0)\n","\n","def cosine_similarity(v1, v2):\n","    return dot(v1, v2) / (norm(v1) * norm(v2))\n","\n","def synonym_insert(sentence, model, num_to_augment=5, min_similarity=0.75):\n","    \"\"\"\n","    Replaces a random word in the sentence with similar word using the provided FastText model.\n","\n","    Args:\n","        sentence (str): The input sentence to augment.\n","        model (fasttext.FastText._FastText): The pre-trained fastText model.\n","        num_to_augment (int): Number of augmented sentences to create to choose from\n","        min_similarity (float): Minimum cosine similarity to consider a word a synonym.\n","\n","    Returns:\n","        str: The augmented sentences\n","    \"\"\"\n","\n","    clean_sentence = _clean_text(sentence)\n","    words = re.findall(r'\\b\\w+\\b', clean_sentence)\n","    if not words:\n","        return None\n","\n","    # Choose a word that exists in the model's vocabulary\n","    candidate_words = [word for word in words if word.lower() in model.words]\n","    if not candidate_words:\n","        print(\"No words in model\")\n","        return None\n","\n","    augmentation_choices = []\n","\n","    for i in range(num_to_augment):\n","      word_to_replace = random.choice(candidate_words)\n","      neighbors = model.get_nearest_neighbors(word_to_replace)\n","\n","      to_insert = random.choice(neighbors)\n","      if isinstance(to_insert, tuple):\n","        to_insert = to_insert[1]\n","\n","      insert_index = random.randint(0, len(words))\n","\n","      words.insert(insert_index, to_insert)\n","\n","      augmented_sentence = ' '.join(words)\n","      augmentation_choices.append(augmented_sentence)\n","\n","    best_cosine = -5\n","    best_augmentation = None\n","    original_vector = sentence_vector(sentence, model)\n","\n","    for augmentation in augmentation_choices:\n","      augmented_vector = sentence_vector(augmentation, model)\n","      similarity = cosine_similarity(original_vector, augmented_vector)\n","\n","      if similarity \u003e best_cosine:\n","        best_cosine = similarity\n","        best_augmentation = augmentation\n","\n","    if best_cosine \u003c min_similarity:\n","      return sentence\n","\n","    return best_augmentation\n","\n","def get_label_counts(dataset):\n","  # Count each individual emotion\n","  emotion_counter = Counter()\n","  df = dataset.to_pandas()\n","\n","  # Iterate through each row in the DataFrame\n","  for emotions_list in df['emotions']:\n","      # Add each emotion to the counter\n","      for emotion in emotions_list:\n","          emotion_counter[emotion] += 1\n","\n","  # Display results\n","  return emotion_counter\n","\n","def get_under_represented_labels(label_counts):\n","  max_count = max(label_counts.values())\n","  threshold = 0.7 * max_count\n","\n","  underrepresented = [label for label, count in label_counts.items() if count \u003c threshold]\n","  return underrepresented, max_count, threshold\n","\n","def _clean_text(text):\n","  \"\"\"\n","  Basic text cleaning - language agnostic\n","  \"\"\"\n","  # Remove extra whitespace\n","  text = re.sub(r'\\s+', ' ', text.strip())\n","  return text\n","\n","def augment_sentence(instance, emotion_label, num_to_generate, model):\n","  \"\"\"\n","  Apply both synonym replacement and random insertion\n","  Following the paper's approach\n","  \"\"\"\n","\n","  sentence = instance[\"text\"]\n","\n","  augmentations = []\n","  for i in range(num_to_generate):\n","    augmented = synonym_insert(sentence, model)\n","    if augmented == sentence:\n","      continue\n","    for a in augmentations:\n","      if augmented == a[\"text\"]:\n","        continue\n","    if augmented is not None:\n","      new_instance = instance.copy()\n","      new_instance[\"text\"] = augmented\n","      augmentations.append(new_instance)\n","\n","  # print(f\"Original: {sentence}\")\n","  # print(f\"Augmented: {augmentations}\")\n","  return augmentations\n","\n","def get_label_aug_percentages(train_dataset, lang):\n","\n","  aug_per_label = []\n","\n","  print(f\"Total number of training instances before augmentation: {len(train_dataset)}\")\n","  print(f\"Training distribution before augmentation:\")\n","  label_counts = get_label_counts(train_dataset)\n","  number_of_labels = len(label_counts)\n","  ideal_distribution = 1 / number_of_labels\n","\n","  for emotion, count in label_counts.items():\n","    percentage = count/len(train_dataset)\n","    print(f\"{emotion}: {count} - {round(percentage*100)}%\")\n","    if percentage \u003e= 1.5 * ideal_distribution:\n","      print(f\"Label {emotion} is over represented\")\n","      aug_per_label.append({\"label\": emotion, \"to_augment\": 1.0})\n","    if percentage \u003e= ideal_distribution:\n","      print(f\"Label {emotion} is well represented\")\n","      aug_per_label.append({\"label\": emotion, \"to_augment\": 1.3})\n","    elif (ideal_distribution - percentage) \u003c= 0.05:\n","      print(f\"Label {emotion} is moderately represented\")\n","      aug_per_label.append({\"label\": emotion, \"to_augment\": 2.0})\n","    elif (ideal_distribution - percentage) \u003c= 0.10:\n","      print(f\"Label {emotion} is underrepresented\")\n","      aug_per_label.append({\"label\": emotion, \"to_augment\": 3.0})\n","    else:\n","      print(f\"Label {emotion} is extremely underrepresented\")\n","      aug_per_label.append({\"label\": emotion, \"to_augment\": 3.0})\n","\n","  return aug_per_label, label_counts\n","\n","def augment_datasets(datasets, language_models):\n","  if run_baseline:\n","      print(\"Running baseline model, skipping data augmentation.\")\n","      return datasets\n","\n","  augmented_datasets = {} # Use a new dictionary to store augmented datasets\n","\n","  for lang, dataset in datasets.items():\n","      print(f\"\\n-------------- Applying data augmentation for {lang} --------------\")\n","      training_set = dataset[\"train\"]\n","      validation_set = dataset[\"validation\"] # Keep original validation and test sets\n","      test_set = dataset[\"test\"]\n","\n","      augmentation_per_label, label_counts = get_label_aug_percentages(training_set, lang)\n","\n","      augmented_training_list = training_set.to_list()\n","\n","      for apl in augmentation_per_label:\n","          label = apl[\"label\"]\n","          aug_amount = apl[\"to_augment\"]\n","          target_count = label_counts[label] * aug_amount\n","          current_count = label_counts[label]\n","\n","          print(f\"Augmenting label: '{label}'...\")\n","          if current_count \u003e= target_count:\n","              print(f\"Label '{label}' already has {current_count} samples, no augmentation needed.\")\n","              continue\n","\n","          num_to_generate = int(target_count - current_count)\n","          augmentations_per_instance = min(math.ceil(num_to_generate/current_count), 4)\n","          print(f\"Generating approx. {num_to_generate} samples for '{label}'...\")\n","\n","          original_samples_for_label = []\n","          for sample in training_set:\n","              if label in sample['emotions']:\n","                  # Store the sample dictionary as is, we'll modify text later\n","                  original_samples_for_label.append(sample)\n","\n","          if not original_samples_for_label:\n","              print(f\"Warning: No samples found with label '{label}' for augmentation.\")\n","              continue\n","\n","          # Generate augmented samples\n","          augmented_samples_list = []\n","          num_augmented = 0\n","\n","          for instance in original_samples_for_label:\n","            if num_augmented \u003e= num_to_generate:\n","              break\n","\n","            augmentations = augment_sentence(instance, label, augmentations_per_instance, language_models[lang])\n","            augmented_samples_list.extend(augmentations)\n","            num_augmented += len(augmentations)\n","\n","          # Add generated samples to the list of training samples\n","          augmented_training_list.extend(augmented_samples_list)\n","\n","      for i, item in enumerate(augmented_training_list):\n","        if not isinstance(item, dict):\n","            print(f\"Item {i} is not a dict:\", item)\n","      # Convert the list back to a Hugging Face Dataset\n","\n","      augmented_train_dataset = Dataset.from_list(augmented_training_list)\n","\n","      print(f\"Total number of training instances after augmentation: {len(augmented_training_list)}\")\n","      print(f\"\\nTraining distribution after augmentation for {lang}\")\n","      augmented_label_counts = get_label_counts(augmented_train_dataset)\n","      for emotion, count in augmented_label_counts.items():\n","        percentage = count/len(augmented_train_dataset)\n","        print(f\"{emotion}: {count} - {round(percentage*100)}\")\n","\n","\n","      # Store the augmented train dataset and original validation/test sets\n","      augmented_datasets[lang] = {\n","          \"train\": augmented_train_dataset,\n","          \"test\": test_set,\n","          \"validation\": validation_set\n","      }\n","\n","  return augmented_datasets\n","\n","\n","augmented_datasets = augment_datasets(preprocessed_datasets.copy(), language_models)"]},{"cell_type":"markdown","metadata":{"id":"v2NSaaA72r-5"},"source":["# Tokenize the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m9D_MpCO2r-5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ceb046327944097aa44064bd9b89e23","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/536 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00a6415e53f84df090e47b2d12e9579f","version_major":2,"version_minor":0},"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/1.65M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efbf190c96e046c884b9f15b6dcc6fdf","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.71M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b12bc53e50546e2808c65c843626a22","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/238 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing dataset for afr...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e64b8205393406891848f58db673be5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5154 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f104fc92ec34391a20b73b302264bcc","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/355 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d83ccdb5d5b34d13a3f9a76e16f2ef32","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/355 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenized dataset for afr\n","Tokenizing dataset for hau...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0db238f53804bb3933359074b774ee4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5863 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42d2343570834c1ba27521184d620db0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/502 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff35c4f38e044d5a80e3e2c68ff9e932","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/502 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenized dataset for hau\n","Tokenizing dataset for swa...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af7eabfb1b8943e8a0a31a86a16bf1bb","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10522 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f24ab82014fa41e3adef4984c9efb167","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/772 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a546d88fbd53470fad457714e66cf0f2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/773 [00:00\u003c?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenized dataset for swa\n"]}],"source":["xmlr_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","def preprocess(batch):\n","    return xmlr_tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n","\n","tokenized_datasets = {}\n","for lang, dataset in augmented_datasets.items():\n","    print(f\"Tokenizing dataset for {lang}...\")\n","    tokenized_datasets[lang] = {}\n","    tokenized_datasets[lang]['train'] = dataset['train'].map(preprocess, batched=True)\n","    tokenized_datasets[lang]['test'] = dataset['test'].map(preprocess, batched=True)\n","    tokenized_datasets[lang]['validation'] = dataset['validation'].map(preprocess, batched=True)\n","    # Set the format for PyTorch\n","    tokenized_datasets[lang]['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","    tokenized_datasets[lang]['test'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","    tokenized_datasets[lang]['validation'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","    print(f\"Tokenized dataset for {lang}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kwg-MBbUgiMF"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogM4WApn2r-6"},"outputs":[],"source":["# Mignon added\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n","    hamming_loss,\n","    multilabel_confusion_matrix\n",")\n","import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n","    probs = sigmoid(logits)\n","    preds = (probs \u003e= 0.3).astype(int)\n","\n","    # Get multilabel confusion matrix: shape (num_labels, 2, 2)\n","    mcm = multilabel_confusion_matrix(labels, preds)\n","\n","    # Convert to dict for logging/inspection\n","    confusion_dict = {\n","        f\"label_{i}\": {\n","            \"tn\": int(cm[0][0]),\n","            \"fp\": int(cm[0][1]),\n","            \"fn\": int(cm[1][0]),\n","            \"tp\": int(cm[1][1]),\n","        }\n","        for i, cm in enumerate(mcm)\n","    }\n","\n","    return {\n","        \"subset_accuracy\": accuracy_score(labels, preds),\n","        \"hamming_loss\": hamming_loss(labels, preds),\n","        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"macro_precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"micro_precision\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"macro_recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"micro_recall\": recall_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"confusion_matrix\": confusion_dict\n","    }\n","\n","def train_finetune(train_data, val_data, num_labels, lang):\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=num_labels,\n","        problem_type=\"multi_label_classification\"  # Set problem type for multi-label\n","    )\n","\n","    args = TrainingArguments(\n","        output_dir=f\"{model_name}-{lang}-ft\",\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=5,\n","        save_strategy=\"no\",\n","        report_to=\"none\",\n","        logging_dir=None,\n","        seed=seed,  # Add seed to training arguments\n","        data_seed=seed,  # Add data seed for data loading\n","        dataloader_num_workers=0,  # Ensure deterministic data loading\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=args,\n","        train_dataset=train_data,\n","        eval_dataset=val_data,\n","        compute_metrics=compute_metrics\n","    )\n","    trainer.train()\n","    return trainer\n","\n","\n","trainers = {}\n","for lang, datasets in augmented_datasets.items():\n","    print(f\"\\nTraining on {lang} dataset...\")\n","\n","    num_labels = len(emotion_labels)\n","    set_all_seeds(seed)\n","    trainers[lang] = train_finetune(tokenized_datasets[lang][\"train\"],tokenized_datasets[lang][\"validation\"], num_labels, lang)\n","    results = trainers[lang].evaluate(tokenized_datasets[lang][\"validation\"], metric_key_prefix=\"eval\")\n","    print(\"Training Fine-tuning Accuracy:\", results['eval_subset_accuracy'])\n","    print(\"Training Hamming Loss:\", results['eval_hamming_loss'])\n","    print(\"Macro F1 Score\", results['eval_macro_f1'])\n","    print(\"Micro F1 Score\", results['eval_micro_f1'])\n","    print(\"Micro Precision\", results['eval_micro_precision'])\n","    print(\"Micro Recall\", results['eval_micro_recall'])\n","    print(\"Macro Precision\", results['eval_macro_precision'])\n","    print(\"Macro Recall\", results['eval_macro_recall'])\n","    print(\"Confusion Matrix\", results['eval_confusion_matrix'])"]},{"cell_type":"markdown","metadata":{"id":"wpb1-1m82r-6"},"source":["# Test Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Yg8ruS52r-7"},"outputs":[],"source":["def test_model(trainer, test_data):\n","    # Reuse the already loaded tokenizer\n","    test_results = trainer.evaluate(\n","        test_data,\n","        metric_key_prefix=\"test\"\n","    )\n","\n","    print(\"Test Accuracy:\", test_results['test_subset_accuracy'])\n","    print(\"Test Hamming Loss:\", test_results['test_hamming_loss'])\n","    print(\"Test Macro F1 Score:\", test_results['test_macro_f1'])\n","    print(\"Test Micro F1 Score:\", test_results['test_micro_f1'])\n","    print(\"Test Micro Precision:\", test_results['test_micro_precision'])\n","    print(\"Test Micro Recall:\", test_results['test_micro_recall'])\n","    print(\"Test Macro Precision:\", test_results['test_macro_precision'])\n","    print(\"Test Macro Recall:\", test_results['test_macro_recall'])\n","\n","    return test_results\n","\n","for lang, datasets in augmented_datasets.items():\n","    print(f\"\\nTesting {lang} model on test {lang} dataset...\")\n","    test_results = test_model(trainers[lang], tokenized_datasets[lang][\"test\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yUoRdAycBLU"},"outputs":[],"source":["from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_multilabel_roc(trainer, dataset, emotion_labels, lang):\n","    # Get raw predictions and labels\n","    outputs = trainer.predict(dataset)\n","    logits = outputs.predictions\n","    labels = outputs.label_ids\n","\n","    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n","    probs = sigmoid(logits)  # Shape: [num_samples, num_labels]\n","\n","    num_labels = len(emotion_labels)\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","\n","    for i in range(num_labels):\n","        fpr[i], tpr[i], _ = roc_curve(labels[:, i], probs[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    label_names = [None] * len(emotion_labels)\n","    for k, v in emotion_labels.items():\n","        label_names[v] = k\n","\n","    # Plot\n","    plt.figure(figsize=(10, 8))\n","    for i in range(num_labels):\n","        plt.plot(fpr[i], tpr[i], label=f\"{label_names[i]} (AUC = {roc_auc[i]:.2f})\")\n","\n","    plt.plot([0, 1], [0, 1], \"k--\")\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(f\"ROC Curves for Language: {lang}\")\n","    plt.legend(loc=\"lower right\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omJG2K_FcCqP"},"outputs":[],"source":["for lang in trainers:\n","    print(f\"Plotting ROC curve for {lang}\")\n","    plot_multilabel_roc(trainers[lang], tokenized_datasets[lang][\"test\"], emotion_labels, lang)"]},{"cell_type":"markdown","metadata":{"id":"K5d1at-I_Ipz"},"source":["# Parameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BtbJ5RD8FlP"},"outputs":[],"source":["# param_grid = {\n","#     \"learning_rate\": [2e-5, 3e-5, 5e-5],\n","#     \"batch_size\": [8, 16],\n","#     \"epochs\": [3, 5],\n","#     \"threshold\": [0.3, 0.5, 0.7]\n","# }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"firGzsb6_NrL"},"outputs":[],"source":["# def train_finetune(model_ckpt, train_data, test_data, num_labels, lr, batch_size, epochs, threshold):\n","#     model = AutoModelForSequenceClassification.from_pretrained(\n","#         model_ckpt,\n","#         num_labels=num_labels,\n","#         problem_type=\"multi_label_classification\"\n","#     )\n","\n","\n","#     args = TrainingArguments(\n","#         output_dir=f\"{model_ckpt}-finetuned\",\n","#         per_device_train_batch_size=batch_size,\n","#         per_device_eval_batch_size=batch_size,\n","#         learning_rate=lr,\n","#         num_train_epochs=epochs,\n","#         save_strategy=\"no\",\n","#         report_to=\"none\",\n","#         logging_dir=None,\n","#         seed=seed,  # Add seed to training arguments\n","#         data_seed=seed,  # Add data seed for data loading\n","#         dataloader_num_workers=0,  # Ensure deterministic data loading\n","#     )\n","\n","#     def compute_metrics(eval_pred):\n","#         logits, labels = eval_pred\n","#         sigmoid = lambda x: 1 / (1 + np.exp(-x))\n","#         probs = sigmoid(logits)\n","#         preds = (probs \u003e= threshold).astype(int)\n","\n","#         return {\n","#             \"subset_accuracy\": accuracy_score(labels, preds),\n","#             \"hamming_loss\": hamming_loss(labels, preds),\n","#             \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n","#             \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n","#         }\n","\n","#     trainer = Trainer(\n","#         model=model,\n","#         args=args,\n","#         train_dataset=train_data,\n","#         eval_dataset=test_data,\n","#         compute_metrics=compute_metrics\n","#     )\n","\n","#     trainer.train()\n","#     results = trainer.evaluate()\n","#     return results\n"]},{"cell_type":"markdown","metadata":{"id":"rKlFoljD_MBs"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPNtPW5A_WcX"},"outputs":[],"source":["# import itertools\n","# import numpy as np\n","# from collections import defaultdict\n","\n","# # Dictionary to store results for each language\n","# language_results = {}\n","# # Dictionary to track average performance across languages for each parameter combination\n","# param_avg_scores = defaultdict(float)\n","# # To count how many languages we test on (for averaging)\n","# num_languages = len(languages)\n","\n","# # Create a compact parameter key format for readability\n","# def param_key(lr, batch_size, epochs, threshold):\n","#     return f\"lr={lr}_bs={batch_size}_ep={epochs}_th={threshold}\"\n","\n","# print(f\"Parameter tuning across {num_languages} languages: {', '.join(languages)}\")\n","\n","# num_labels = len(emotion_labels)\n","# # Generate all parameter combinations once\n","# param_combinations = list(itertools.product(\n","#     param_grid[\"learning_rate\"],\n","#     param_grid[\"batch_size\"],\n","#     param_grid[\"epochs\"],\n","#     param_grid[\"threshold\"]\n","# ))\n","\n","# # First, train and evaluate on each language separately\n","# for lang in languages:\n","#     print(f\"\\n{'='*50}\")\n","#     print(f\"TUNING ON LANGUAGE: {lang}\")\n","#     print(f\"{'='*50}\")\n","\n","#     language_results[lang] = []\n","#     best_lang_score = 0\n","#     best_lang_params = {}\n","\n","#     for lr, batch_size, epochs, threshold in param_combinations:\n","#         print(f\"\\nTesting on {lang}: lr={lr}, batch_size={batch_size}, epochs={epochs}, threshold={threshold}\")\n","\n","#         set_all_seeds(seed)\n","#         # Train and evaluate on this language with these parameters\n","#         results = train_finetune(\n","#             \"Davlan/afro-xlmr-small\",\n","#             tokenized_datasets[lang][\"train\"],\n","#             tokenized_datasets[lang][\"validation\"],\n","#             num_labels, lr, batch_size, epochs, threshold\n","#         )\n","\n","#         # Extract the score (using micro_f1 as our metric)\n","#         score = results[\"eval_micro_f1\"]\n","#         print(f\"{lang} Micro-F1 Score: {score:.4f}\")\n","\n","#         # Store results for this language\n","#         language_results[lang].append((score, lr, batch_size, epochs, threshold))\n","\n","#         # Update best parameters for this language\n","#         if score \u003e best_lang_score:\n","#             best_lang_score = score\n","#             best_lang_params = {\n","#                 \"learning_rate\": lr,\n","#                 \"batch_size\": batch_size,\n","#                 \"epochs\": epochs,\n","#                 \"threshold\": threshold\n","#             }\n","\n","#         # Add to our running average across languages\n","#         param_key_str = param_key(lr, batch_size, epochs, threshold)\n","#         param_avg_scores[param_key_str] += score / num_languages\n","\n","#     # Print the best parameters for this language\n","#     print(f\"\\n✅ Best parameters for {lang}:\")\n","#     print(best_lang_params)\n","#     print(f\"Best {lang} Micro-F1 Score: {best_lang_score:.4f}\")\n","\n","# # Find the parameter combination with the best average performance across languages\n","# best_avg_score = 0\n","# best_avg_params = None\n","\n","# for params, avg_score in param_avg_scores.items():\n","#     if avg_score \u003e best_avg_score:\n","#         best_avg_score = avg_score\n","#         best_avg_params = params\n","\n","# # Parse the parameter key back into a dictionary\n","# lr, batch_size, epochs, threshold = best_avg_params.split(\"_\")\n","# best_params = {\n","#     \"learning_rate\": float(lr.split('=')[1]),\n","#     \"batch_size\": int(batch_size.split('=')[1]),\n","#     \"epochs\": int(epochs.split('=')[1]),\n","#     \"threshold\": float(threshold.split('=')[1])\n","# }\n","\n","# print(\"\\n\" + \"=\"*70)\n","# print(\"FINAL RESULTS ACROSS ALL LANGUAGES\")\n","# print(\"=\"*70)\n","# print(\"\\n✅ Best overall parameters (averaged across all languages):\")\n","# print(best_params)\n","# print(f\"Average Micro-F1 Score: {best_avg_score:.4f}\")\n","\n","# # Create a performance matrix to visualize results\n","# print(\"\\nPerformance across languages:\")\n","# print(f\"{'Parameters':\u003c40} | {'Average':\u003c10} | \" + \" | \".join([f\"{lang:\u003c10}\" for lang in languages]))\n","# print(\"-\" * (50 + 12 * num_languages))\n","\n","# # Sort parameter combinations by average score for better readability\n","# sorted_params = sorted(param_avg_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","# # Print top 5 parameter combinations\n","# for i, (params, avg_score) in enumerate(sorted_params[:5]):\n","#     # Get individual language scores for this parameter combination\n","#     lang_scores = []\n","#     for lang in languages:\n","#         # Find the score for this parameter combination in this language\n","#         for result in language_results[lang]:\n","#             score, lr, bs, ep, th = result\n","#             if param_key(lr, bs, ep, th) == params:\n","#                 lang_scores.append(score)\n","#                 break\n","\n","#     # Print the row\n","#     print(f\"{params:\u003c40} | {avg_score:.4f}      | \" + \" | \".join([f\"{score:.4f}      \" for score in lang_scores]))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}